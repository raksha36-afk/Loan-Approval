{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90b9bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9882116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('loan_approval_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecedce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "# Drop the 'loan_id' column as it is not needed for training\n",
    "data.drop('loan_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa897c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in [' education', ' self_employed']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "221f8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable\n",
    "data[' loan_status'] = LabelEncoder().fit_transform(data[' loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54921f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = data.drop(' loan_status', axis=1)\n",
    "y = data[' loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dca9dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23105535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "464ad456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\91944\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11c0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bc17e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1224 - val_accuracy: 0.9678 - val_loss: 0.0910\n",
      "Epoch 2/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.1061 - val_accuracy: 0.9663 - val_loss: 0.1028\n",
      "Epoch 3/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9570 - loss: 0.1057 - val_accuracy: 0.9707 - val_loss: 0.0971\n",
      "Epoch 4/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9681 - loss: 0.0966 - val_accuracy: 0.9707 - val_loss: 0.0904\n",
      "Epoch 5/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9663 - loss: 0.1132 - val_accuracy: 0.9736 - val_loss: 0.0803\n",
      "Epoch 6/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.0941 - val_accuracy: 0.9678 - val_loss: 0.0881\n",
      "Epoch 7/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9657 - loss: 0.0935 - val_accuracy: 0.9722 - val_loss: 0.0908\n",
      "Epoch 8/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9688 - loss: 0.0983 - val_accuracy: 0.9707 - val_loss: 0.0883\n",
      "Epoch 9/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.1001 - val_accuracy: 0.9707 - val_loss: 0.0919\n",
      "Epoch 10/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9711 - loss: 0.1008 - val_accuracy: 0.9663 - val_loss: 0.0816\n",
      "Epoch 11/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9607 - loss: 0.1018 - val_accuracy: 0.9707 - val_loss: 0.0898\n",
      "Epoch 12/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.0947 - val_accuracy: 0.9678 - val_loss: 0.0890\n",
      "Epoch 13/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9651 - loss: 0.0991 - val_accuracy: 0.9693 - val_loss: 0.0833\n",
      "Epoch 14/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9651 - loss: 0.1034 - val_accuracy: 0.9707 - val_loss: 0.0839\n",
      "Epoch 15/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9726 - loss: 0.0813 - val_accuracy: 0.9722 - val_loss: 0.0848\n",
      "Epoch 16/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9669 - loss: 0.1008 - val_accuracy: 0.9707 - val_loss: 0.0788\n",
      "Epoch 17/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9685 - loss: 0.0903 - val_accuracy: 0.9707 - val_loss: 0.0823\n",
      "Epoch 18/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9740 - loss: 0.0873 - val_accuracy: 0.9678 - val_loss: 0.0904\n",
      "Epoch 19/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.0989 - val_accuracy: 0.9678 - val_loss: 0.0855\n",
      "Epoch 20/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.0960 - val_accuracy: 0.9707 - val_loss: 0.0802\n",
      "Epoch 21/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9720 - loss: 0.0884 - val_accuracy: 0.9766 - val_loss: 0.0765\n",
      "Epoch 22/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9606 - loss: 0.1060 - val_accuracy: 0.9707 - val_loss: 0.0784\n",
      "Epoch 23/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9672 - loss: 0.0971 - val_accuracy: 0.9678 - val_loss: 0.0810\n",
      "Epoch 24/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0878 - val_accuracy: 0.9693 - val_loss: 0.0857\n",
      "Epoch 25/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9644 - loss: 0.0920 - val_accuracy: 0.9663 - val_loss: 0.0862\n",
      "Epoch 26/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9634 - loss: 0.1009 - val_accuracy: 0.9678 - val_loss: 0.0823\n",
      "Epoch 27/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0766 - val_accuracy: 0.9722 - val_loss: 0.0884\n",
      "Epoch 28/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0775 - val_accuracy: 0.9707 - val_loss: 0.0823\n",
      "Epoch 29/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0874 - val_accuracy: 0.9736 - val_loss: 0.0821\n",
      "Epoch 30/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9765 - loss: 0.0801 - val_accuracy: 0.9751 - val_loss: 0.0873\n",
      "Epoch 31/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.0747 - val_accuracy: 0.9736 - val_loss: 0.0779\n",
      "Epoch 32/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.0972 - val_accuracy: 0.9663 - val_loss: 0.0920\n",
      "Epoch 33/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.0897 - val_accuracy: 0.9693 - val_loss: 0.0900\n",
      "Epoch 34/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9690 - loss: 0.0915 - val_accuracy: 0.9678 - val_loss: 0.0959\n",
      "Epoch 35/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.0811 - val_accuracy: 0.9707 - val_loss: 0.0914\n",
      "Epoch 36/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9734 - loss: 0.0837 - val_accuracy: 0.9707 - val_loss: 0.0797\n",
      "Epoch 37/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9612 - loss: 0.0931 - val_accuracy: 0.9707 - val_loss: 0.0914\n",
      "Epoch 38/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.0761 - val_accuracy: 0.9736 - val_loss: 0.0783\n",
      "Epoch 39/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9715 - loss: 0.0831 - val_accuracy: 0.9722 - val_loss: 0.0840\n",
      "Epoch 40/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0794 - val_accuracy: 0.9693 - val_loss: 0.0927\n",
      "Epoch 41/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0746 - val_accuracy: 0.9707 - val_loss: 0.0932\n",
      "Epoch 42/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0825 - val_accuracy: 0.9736 - val_loss: 0.0884\n",
      "Epoch 43/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9774 - loss: 0.0632 - val_accuracy: 0.9707 - val_loss: 0.0902\n",
      "Epoch 44/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0913 - val_accuracy: 0.9722 - val_loss: 0.0902\n",
      "Epoch 45/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9713 - loss: 0.0789 - val_accuracy: 0.9736 - val_loss: 0.0846\n",
      "Epoch 46/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9687 - loss: 0.0875 - val_accuracy: 0.9736 - val_loss: 0.0829\n",
      "Epoch 47/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0740 - val_accuracy: 0.9707 - val_loss: 0.0960\n",
      "Epoch 48/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9736 - loss: 0.0826 - val_accuracy: 0.9663 - val_loss: 0.0942\n",
      "Epoch 49/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9751 - loss: 0.0751 - val_accuracy: 0.9678 - val_loss: 0.0883\n",
      "Epoch 50/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9726 - loss: 0.0856 - val_accuracy: 0.9693 - val_loss: 0.0933\n",
      "Epoch 51/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0809 - val_accuracy: 0.9693 - val_loss: 0.0930\n",
      "Epoch 52/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.0821 - val_accuracy: 0.9678 - val_loss: 0.0940\n",
      "Epoch 53/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.0786 - val_accuracy: 0.9693 - val_loss: 0.1009\n",
      "Epoch 54/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0820 - val_accuracy: 0.9707 - val_loss: 0.0878\n",
      "Epoch 55/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9707 - loss: 0.0737 - val_accuracy: 0.9649 - val_loss: 0.0844\n",
      "Epoch 56/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9710 - loss: 0.0845 - val_accuracy: 0.9649 - val_loss: 0.0939\n",
      "Epoch 57/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0811 - val_accuracy: 0.9722 - val_loss: 0.0889\n",
      "Epoch 58/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9729 - loss: 0.0774 - val_accuracy: 0.9678 - val_loss: 0.0879\n",
      "Epoch 59/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0745 - val_accuracy: 0.9722 - val_loss: 0.0948\n",
      "Epoch 60/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0766 - val_accuracy: 0.9678 - val_loss: 0.0888\n",
      "Epoch 61/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0788 - val_accuracy: 0.9663 - val_loss: 0.0913\n",
      "Epoch 62/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0749 - val_accuracy: 0.9634 - val_loss: 0.0986\n",
      "Epoch 63/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0925 - val_accuracy: 0.9707 - val_loss: 0.0859\n",
      "Epoch 64/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9654 - loss: 0.0985 - val_accuracy: 0.9707 - val_loss: 0.0973\n",
      "Epoch 65/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9725 - loss: 0.0842 - val_accuracy: 0.9678 - val_loss: 0.1087\n",
      "Epoch 66/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0755 - val_accuracy: 0.9722 - val_loss: 0.1020\n",
      "Epoch 67/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0784 - val_accuracy: 0.9707 - val_loss: 0.0954\n",
      "Epoch 68/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9746 - loss: 0.0790 - val_accuracy: 0.9722 - val_loss: 0.0953\n",
      "Epoch 69/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9772 - loss: 0.0757 - val_accuracy: 0.9707 - val_loss: 0.0933\n",
      "Epoch 70/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9725 - loss: 0.0751 - val_accuracy: 0.9634 - val_loss: 0.0954\n",
      "Epoch 71/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9737 - loss: 0.0820 - val_accuracy: 0.9707 - val_loss: 0.0864\n",
      "Epoch 72/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.0912 - val_accuracy: 0.9722 - val_loss: 0.0770\n",
      "Epoch 73/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9777 - loss: 0.0637 - val_accuracy: 0.9736 - val_loss: 0.0804\n",
      "Epoch 74/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9808 - loss: 0.0649 - val_accuracy: 0.9722 - val_loss: 0.0876\n",
      "Epoch 75/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9759 - loss: 0.0806 - val_accuracy: 0.9736 - val_loss: 0.0821\n",
      "Epoch 76/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9730 - loss: 0.0752 - val_accuracy: 0.9722 - val_loss: 0.0842\n",
      "Epoch 77/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9763 - loss: 0.0709 - val_accuracy: 0.9678 - val_loss: 0.0858\n",
      "Epoch 78/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9725 - loss: 0.0736 - val_accuracy: 0.9707 - val_loss: 0.0920\n",
      "Epoch 79/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9834 - loss: 0.0620 - val_accuracy: 0.9722 - val_loss: 0.0793\n",
      "Epoch 80/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9743 - loss: 0.0699 - val_accuracy: 0.9693 - val_loss: 0.0940\n",
      "Epoch 81/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9654 - loss: 0.0960 - val_accuracy: 0.9678 - val_loss: 0.0952\n",
      "Epoch 82/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9795 - loss: 0.0611 - val_accuracy: 0.9736 - val_loss: 0.0970\n",
      "Epoch 83/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9740 - loss: 0.0711 - val_accuracy: 0.9722 - val_loss: 0.0941\n",
      "Epoch 84/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9780 - loss: 0.0631 - val_accuracy: 0.9736 - val_loss: 0.1024\n",
      "Epoch 85/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.0652 - val_accuracy: 0.9707 - val_loss: 0.1057\n",
      "Epoch 86/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0709 - val_accuracy: 0.9722 - val_loss: 0.0862\n",
      "Epoch 87/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9732 - loss: 0.0834 - val_accuracy: 0.9693 - val_loss: 0.0883\n",
      "Epoch 88/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0723 - val_accuracy: 0.9707 - val_loss: 0.0849\n",
      "Epoch 89/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9724 - loss: 0.0887 - val_accuracy: 0.9766 - val_loss: 0.0914\n",
      "Epoch 90/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9803 - loss: 0.0609 - val_accuracy: 0.9751 - val_loss: 0.0918\n",
      "Epoch 91/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9753 - loss: 0.0672 - val_accuracy: 0.9707 - val_loss: 0.0969\n",
      "Epoch 92/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9780 - loss: 0.0686 - val_accuracy: 0.9722 - val_loss: 0.1028\n",
      "Epoch 93/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 0.0561 - val_accuracy: 0.9736 - val_loss: 0.0914\n",
      "Epoch 94/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9794 - loss: 0.0723 - val_accuracy: 0.9736 - val_loss: 0.0876\n",
      "Epoch 95/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0826 - val_accuracy: 0.9736 - val_loss: 0.0905\n",
      "Epoch 96/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9778 - loss: 0.0597 - val_accuracy: 0.9722 - val_loss: 0.0898\n",
      "Epoch 97/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9770 - loss: 0.0572 - val_accuracy: 0.9736 - val_loss: 0.0858\n",
      "Epoch 98/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9757 - loss: 0.0801 - val_accuracy: 0.9736 - val_loss: 0.0843\n",
      "Epoch 99/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9711 - loss: 0.0783 - val_accuracy: 0.9707 - val_loss: 0.0829\n",
      "Epoch 100/100\n",
      "\u001b[1m86/86\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9720 - loss: 0.0736 - val_accuracy: 0.9678 - val_loss: 0.0882\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b4eb21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9738 - loss: 0.0667\n",
      "Test Accuracy: 97.31%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baebf273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting the results\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54c59763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[532   4]\n",
      " [ 19 299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       536\n",
      "           1       0.99      0.94      0.96       318\n",
      "\n",
      "    accuracy                           0.97       854\n",
      "   macro avg       0.98      0.97      0.97       854\n",
      "weighted avg       0.97      0.97      0.97       854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1eb7bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as mlp_loan_approval_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('mlp.h5')\n",
    "print(\"Model saved as mlp_loan_approval_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b3c0ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the module for LimeTabularExplainer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lime_tabular\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiating the explainer object by passing in the training set,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# and the extracted features\u001b[39;00m\n\u001b[0;32m      6\u001b[0m explainer_lime \u001b[38;5;241m=\u001b[39m lime_tabular\u001b[38;5;241m.\u001b[39mLimeTabularExplainer(X_train,\n\u001b[0;32m      7\u001b[0m                                                    feature_names\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m      8\u001b[0m                                                    verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[0;32m      9\u001b[0m                                                    mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "# Importing the module for LimeTabularExplainer\n",
    "from lime import lime_tabular\n",
    " \n",
    "# Instantiating the explainer object by passing in the training set,\n",
    "# and the extracted features\n",
    "explainer_lime = lime_tabular.LimeTabularExplainer(X_train,\n",
    "                                                   feature_names=features,\n",
    "                                                   verbose=True, \n",
    "                                                   mode='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916efd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index corresponding to the test vector\n",
    "i = 10\n",
    "\n",
    "# Number denoting the top features\n",
    "k = 5\n",
    "\n",
    "# Calling the explain_instance method by passing in the:\n",
    "# 1) ith test vector\n",
    "# 2) prediction function used by our prediction model('reg' in this case)\n",
    "# 3) the top features which we want to see, denoted by k\n",
    "\n",
    "exp_lime = explainer_lime.explain_instance(\n",
    "\tX_test[i], reg.predict, num_features=k)\n",
    "\n",
    "# Finally visualizing the explanations\n",
    "exp_lime.show_in_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfdb041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
